{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2fe8d-fde7-4c7a-9e6f-8169ae83ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrickymoron\u001b[0m (\u001b[33mriccardo-moroni\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "\n",
    "from CharRNN import CharRNN\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec9278-a097-4f9a-b309-1a3748447b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "\n",
    "def data_pruning(json_train_path, json_test_path, crop_amount=2000, n_clients=100):\n",
    "    \"\"\"\n",
    "    Reduces the dimension of LEAF dataset.\n",
    "    Samples 'n_clients' clients among those having at least 'crop_amount' training samples.\n",
    "    Each client is given 'crop_amount' number of contigous training samples\n",
    "\n",
    "    Returns:\n",
    "        - 4 dictionaries (X_train, Y_train, X_test, Y_test) \n",
    "    \"\"\"\n",
    "    rand_seed=0\n",
    "    with open(json_train_path) as train_json_data:\n",
    "        train_dict = json.load(train_json_data)\n",
    "    with open(json_test_path) as test_json_data:\n",
    "        test_dict = json.load(test_json_data)\n",
    "\n",
    "    users_complete = train_dict['users']\n",
    "\n",
    "    X_train_cropped, Y_train_cropped, X_test_cropped, Y_test_cropped = {}, {}, {}, {}\n",
    "\n",
    "    i=0\n",
    "    for k in train_dict['user_data'].keys():\n",
    "        if train_dict['num_samples'][i] > crop_amount:\n",
    "            np.random.seed(rand_seed)\n",
    "            start = np.random.randint(len(train_dict['user_data'][k]['x'])-crop_amount)\n",
    "            X_train_cropped[k] = train_dict['user_data'][k]['x'][start:start+crop_amount]\n",
    "            Y_train_cropped[k] = train_dict['user_data'][k]['y'][start:start+crop_amount]\n",
    "            X_test_cropped[k] = test_dict['user_data'][k]['x'][start:start+crop_amount]\n",
    "            Y_test_cropped[k] = test_dict['user_data'][k]['y'][start:start+crop_amount]\n",
    "            rand_seed+=1\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1\n",
    "\n",
    "    users_selected = random.sample(list(X_train_cropped.keys()), n_clients)\n",
    "\n",
    "    X_train = {key: X_train_cropped[key] for key in users_selected}\n",
    "    Y_train = {key: Y_train_cropped[key] for key in users_selected}\n",
    "    X_test = {key: X_test_cropped[key] for key in users_selected}\n",
    "    Y_test = {key: Y_test_cropped[key] for key in users_selected}\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def concat_dict_values(my_dict):\n",
    "    concat = []\n",
    "    for v in my_dict.values():\n",
    "        if isinstance(v, list):\n",
    "            concat.extend(v)\n",
    "        else:\n",
    "            concat.append(v)\n",
    "    return concat\n",
    "\n",
    "json_train_path = '../../datasets/shakespeare/train/all_data_niid_0_keep_0_train_9.json'\n",
    "json_test_path = '../../datasets/shakespeare/test/all_data_niid_0_keep_0_test_9.json'\n",
    "\n",
    "X_train_pruned, Y_train_pruned, X_test_pruned, Y_test_pruned = data_pruning(json_train_path, json_test_path)\n",
    "X_train = concat_dict_values(X_train_pruned)\n",
    "Y_train = concat_dict_values(Y_train_pruned)\n",
    "X_test = concat_dict_values(X_test_pruned)\n",
    "Y_test = concat_dict_values(Y_test_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b40bc-8d3f-4c65-a5b6-2b73bc705bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "train_sentence = ' '.join(X_train)\n",
    "vocab_train = sorted(set(train_sentence))\n",
    "vocab_train.append('<OOV>')\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab_train)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd68c4-c516-4523-816a-ff74abf06dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:01<00:00, 137621.36it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 853065.19it/s]\n",
      "100%|██████████| 3584/3584 [00:00<00:00, 187025.80it/s]\n",
      "100%|██████████| 3584/3584 [00:00<00:00, 677103.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# create a ./tensors/ folder in which to save the (encoded) tensors \n",
    "\n",
    "def tokenize_encode(my_list, char_to_idx):\n",
    "    oov_token = len(vocab_train)-1\n",
    "    new_list = []\n",
    "    for sentence in tqdm(my_list):\n",
    "        characters = list(sentence)\n",
    "\n",
    "        encoded = []\n",
    "        for char in characters:\n",
    "            if char in char_to_idx:\n",
    "                encoded.append(char_to_idx[char])\n",
    "            else:\n",
    "                encoded.append(oov_token)\n",
    "    \n",
    "        new_list.append(encoded)\n",
    "    return new_list\n",
    "\n",
    "X_train_enc = np.array(tokenize_encode(X_train, char_to_idx))\n",
    "Y_train_enc = np.array(tokenize_encode(Y_train, char_to_idx)).flatten()\n",
    "X_test_enc = np.array(tokenize_encode(X_test, char_to_idx))\n",
    "Y_test_enc = np.array(tokenize_encode(Y_test, char_to_idx)).flatten()\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_enc, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(Y_train_enc, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_enc, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test_enc, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098fa69a-47c0-403c-925b-be98d0e9a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# training parameters\n",
    "train_params = {\n",
    "    'batch_size' : 100,\n",
    "    'lr' : 1e-1,\n",
    "    'epochs' : 10,\n",
    "    'momentum': 0.9,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, train_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, train_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f66d2-0e8f-4bd7-8d72-46e42d3936c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\data-science\\courses\\mldl24\\federated_learning_project\\federated_learning_mldl24\\src\\shakespeare\\wandb\\run-20240606_230920-au6jcqfc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riccardo-moroni/fl/runs/au6jcqfc' target=\"_blank\">centralized_shakespeare</a></strong> to <a href='https://wandb.ai/riccardo-moroni/fl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riccardo-moroni/fl' target=\"_blank\">https://wandb.ai/riccardo-moroni/fl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riccardo-moroni/fl/runs/au6jcqfc' target=\"_blank\">https://wandb.ai/riccardo-moroni/fl/runs/au6jcqfc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(69, 8)\n",
      "  (stacked_lstm): LSTM(8, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=69, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# model parameters\n",
    "model_params = {\n",
    "    'vocab_size' : len(vocab_train),\n",
    "    'embed_dim' : 8,\n",
    "    'lstm_units' : 256,\n",
    "}\n",
    "\n",
    "all_params = train_params.copy()\n",
    "all_params.update(model_params)\n",
    "wandb.init(\n",
    "    project='fl',\n",
    "    name=f'centralized_shakespeare',\n",
    "    config= all_params\n",
    ")\n",
    "\n",
    "model = CharRNN(vocab_size = model_params['vocab_size'], embed_dim = model_params['embed_dim'], lstm_units=model_params['lstm_units']).cuda()\n",
    "model.to('cuda')\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = SGD(model.parameters(), lr=train_params['lr'], momentum=train_params['momentum'], weight_decay=4e-4)\n",
    "\n",
    "def train(model):\n",
    "    accuracies, losses = [], []\n",
    "    for t in range(train_params['epochs']):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.save(model.state_dict(), f\"./saved_models/{train_params['epochs']}epochs_weights.pt\")\n",
    "\n",
    "        # test (after each single epoch)\n",
    "        acc, loss = test(model)\n",
    "        wandb.log({'acc': acc, 'loss': loss, 'epoch': t})\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return accuracies, losses\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), desc=f\"Testing...\")\n",
    "        for batch_idx, (inputs, targets) in progress_bar: \n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f'Test Loss: {test_loss:.6f} Acc: {test_accuracy:.2f}%')\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae494d3-0723-4ca6-a626-f85bcaac0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:32<00:00, 60.71it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 149.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.208349 Acc: 38.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 59.90it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 160.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.006232 Acc: 43.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 59.07it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 112.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.925065 Acc: 44.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 59.26it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 167.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.906087 Acc: 45.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 59.44it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 143.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.866817 Acc: 46.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 58.40it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 113.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.845212 Acc: 46.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 58.43it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 154.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.821997 Acc: 47.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 58.77it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 149.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.785317 Acc: 47.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 58.89it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 150.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.802861 Acc: 47.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:32<00:00, 60.80it/s]\n",
      "Testing...: 100%|██████████| 36/36 [00:00<00:00, 125.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.789084 Acc: 47.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "accuracies, losses = train(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
