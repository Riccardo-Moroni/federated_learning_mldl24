{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIx5WEaJuM5I"
      },
      "source": [
        "Paper: https://arxiv.org/abs/2003.00295"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PSmFOKMBMTjP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.ops import MLP\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10, MNIST\n",
        "from torch.optim import SGD\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWh1Xx1WqXbk"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "viVq7PLXnRkI"
      },
      "outputs": [],
      "source": [
        "K = 500\n",
        "S = np.array(range(K))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoAZ8gOdacB5",
        "outputId": "9db745bf-28dc-4dff-fc76-f9b758cb2245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.RandomCrop((24, 24)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10('datasets/cifar10', train=True, transform=preprocess, download=True)\n",
        "test_dataset = CIFAR10('datasets/cifar10', train=False, transform=preprocess, download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YLGe_BkJ2YdR"
      },
      "outputs": [],
      "source": [
        "iid = True\n",
        "samples_per_client = int(len(train_dataset) / K)\n",
        "shards_per_client = 2\n",
        "\n",
        "def split_data(dataset, iid=True):\n",
        "    if iid:\n",
        "        return torch.utils.data.random_split(train_dataset, [samples_per_client] * K)\n",
        "    else:\n",
        "        sorted_dataset = sorted(train_dataset, key=lambda x: x[1])\n",
        "        shard_size = int(samples_per_client / shards_per_client)\n",
        "        shards = [\n",
        "            torch.utils.data.Subset(\n",
        "                sorted_dataset,\n",
        "                range(i*shard_size, (i+1)*shard_size)\n",
        "            )\n",
        "            for i in range(K*shards_per_client)\n",
        "        ]\n",
        "\n",
        "        random.shuffle(shards)\n",
        "\n",
        "        return [\n",
        "            torch.utils.data.ConcatDataset([shards[2*i], shards[2*i+1]])\n",
        "            for i in range(K)\n",
        "        ]\n",
        "\n",
        "\n",
        "client_datasets = split_data(train_dataset, iid)\n",
        "assert len(client_datasets) == K\n",
        "assert len(client_datasets[0]) == samples_per_client\n",
        "assert iid or all([0 < len(set(map(lambda x: x[1], client_datasets[i]))) <= 4 for i in range(K)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCoMOlAqhoy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCFyReyRRmzH",
        "outputId": "df3ee82a-2ecf-4926-bcf2-ef9bdad821ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False).cuda()\n",
        "model.fc = nn.Linear(512, 10)\n",
        "model.to('cuda')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8gzjnPIqpCy"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pD_OTrI6qoMR"
      },
      "outputs": [],
      "source": [
        "def reduce_w(w_list, f):\n",
        "    return OrderedDict([\n",
        "            (key, f([x[key] for x in w_list])) for key in w_list[0].keys()\n",
        "        ])\n",
        "\n",
        "\n",
        "def tensor_sum(tensors_list):\n",
        "    return torch.sum(torch.stack(tensors_list), dim=0)\n",
        "\n",
        "\n",
        "def w_norm2(w):\n",
        "    res = 0\n",
        "    for key in w.keys():\n",
        "        res += torch.linalg.vector_norm(w[key]) ** 2\n",
        "    return math.sqrt(res)\n",
        "\n",
        "\n",
        "def fed_adagrad(v, delta, params):\n",
        "    delta_norm2 = w_norm2(delta)\n",
        "    return v + delta_norm2\n",
        "\n",
        "\n",
        "def fed_yogi(v, delta, params):\n",
        "    delta_norm2 = w_norm2(delta)\n",
        "    return v - (1-params['beta2']) * delta_norm2 * torch.sign(v - delta_norm2)\n",
        "\n",
        "\n",
        "def fed_adam(v, delta, params):\n",
        "    delta_norm2 = w_norm2(delta)\n",
        "    return params['beta2'] * v + (1-params['beta2']) * delta_norm2\n",
        "\n",
        "\n",
        "methods = {\n",
        "    'adagrad': fed_adagrad,\n",
        "    'yogi': fed_yogi,\n",
        "    'adam': fed_adam\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3HCdQRYqe5f"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9RNbgaj0TNS4"
      },
      "outputs": [],
      "source": [
        "T = 200\n",
        "test_freq = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUnGzbKJZscB"
      },
      "outputs": [],
      "source": [
        "def test(model):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    test_accuracy = 100. * correct / total\n",
        "    print(f'Test Loss: {test_loss:.6f} Acc: {test_accuracy:.2f}%')\n",
        "    return test_accuracy\n",
        "\n",
        "\n",
        "def client_update(model, k, params):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=params['lr_client'])\n",
        "    loader = DataLoader(client_datasets[k], batch_size=params['B'])\n",
        "\n",
        "    for i in range(params['E']):\n",
        "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model.state_dict()\n",
        "\n",
        "\n",
        "def train(model, params):\n",
        "    accuracies = []\n",
        "    v = params['tau'] ** 2\n",
        "    w = model.state_dict()\n",
        "    m = reduce_w([w], lambda x: torch.mul(x[0], 0.0))\n",
        "    for t in tqdm(range(T)):\n",
        "        sampled_clients_num = int(max(params['C']*K, 1))\n",
        "        s = np.random.choice(S, sampled_clients_num, replace=False)\n",
        "\n",
        "        w_clients = []\n",
        "        for k in s:\n",
        "            w_clients.append(client_update(copy.deepcopy(model), k, params))\n",
        "\n",
        "        if params['method'] == 'fedavg':\n",
        "            w = reduce_w(\n",
        "                w_clients, \n",
        "                lambda x: tensor_sum(x) / len(w_clients)\n",
        "            )\n",
        "        else:\n",
        "            deltas = [\n",
        "                reduce_w(\n",
        "                    [w, w_client],\n",
        "                    lambda x: x[1] - x[0]\n",
        "                ) for w_client in w_clients\n",
        "            ]\n",
        "\n",
        "            # n_weights = [len(client_datasets[k])/len(w_clients) for k in s]\n",
        "            delta = reduce_w(\n",
        "                deltas,\n",
        "                lambda x: tensor_sum(x) / len(deltas)\n",
        "            )\n",
        "\n",
        "            m = reduce_w(\n",
        "                [m, delta],\n",
        "                lambda x: params['beta1'] * x[0] + (1-params['beta1']) * x[1]\n",
        "            )\n",
        "\n",
        "            v = methods[params['method']](v, delta, params)\n",
        "            w = reduce_w(\n",
        "                [w, m],\n",
        "                lambda x: x[0] + params['lr_server'] * x[1] / (math.sqrt(v) + params['tau'])\n",
        "            )\n",
        "\n",
        "        model.load_state_dict(w)\n",
        "\n",
        "        if t % test_freq == 0 or t == T-1:\n",
        "            accuracies.append(test(model))\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "params = {\n",
        "    'C': 10/K,\n",
        "    'B': 20,\n",
        "    'E': 1,\n",
        "    'lr_server': 1e-1,\n",
        "    'lr_client': 1e-1,\n",
        "    'method': 'adagrad',\n",
        "    'beta1': 0,\n",
        "    'beta2': 0,\n",
        "    'tau': 1e-3\n",
        "}\n",
        "\n",
        "accuracies = train(model, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qwWM9oUxUD"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('rounds')\n",
        "plt.ylabel('accuracy')\n",
        "xx = np.arange(0, T + test_freq, test_freq)\n",
        "plt.plot(xx, accuracies, label=params['method'], marker='.')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
